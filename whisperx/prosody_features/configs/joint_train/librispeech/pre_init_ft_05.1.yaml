random_seed: 42
ckpt_path: "/home1/nmehlman/arts/vpc/logs/tensorboard/librispeech/joint_train/pre_init_ft/version_4/checkpoints/best_model-epoch=186-step=77852-val_accuracy=0.98.ckpt"
gpus: "0,1"

# Dataset options:
data:
  root_path: "/project/shrikann_35/nmehlman/psid_data/librispeech_feats"
  split: "train"
  max_sample_length: 1024
  sr_embed_model: 'wavlm'

# Dataloader options:
dataloader:
  train_batch_size: 64
  val_batch_size: 32
  num_workers: 8
  val_frac: 0.2

# Model options:
lightning:
  sr_fusion: true
  sr_embed_dim: 512
  feature_model_ckpt: "/home1/nmehlman/arts/vpc/logs/tensorboard/librispeech/small_att_more_layers/version_2/checkpoints/best_model-epoch=239-step=199981-val_accuracy=0.34.ckpt"
  freeze_feature_model: false
  prosody_fusion_dim: 256
  sr_fusion_dim: 512
  hparams: 
    d_model: 256
    dropout: 0.2
    embedding_dim: 128
    local_attn_mask: 2
    max_sample_length: 1024
    num_layers: 6
    num_tokens: 28
  optimizer_params:
    lr: 0.001
    weight_decay: 0.00001
  scheduler_params:
    T_max: 300
    eta_min: 0.00001


# Training options:
trainer:
  max_epochs: 300
  val_check_interval: 0.25 # Must be >= 1 for iterable-type dataset
  log_every_n_steps: 10    
  sync_batchnorm: True
  accelerator: "gpu"
  devices: "auto"
  num_nodes: 1 # DEBUG
  deterministic: False
  accumulate_grad_batches: 1

# Tensorboard options:
tensorboard:
  save_dir: "/home1/nmehlman/arts/vpc/logs/tensorboard/tests" # DEBUG
  name: "training_continue" # DEBUG
  version: null     # Automatic versioning if set to null (recommended)


